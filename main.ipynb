{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0bcf4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SnowballStemmer, WordNetLemmatizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Stock Sentiment Analysis - Text Mining Project\n",
    "\n",
    "## Project Structure:\n",
    "# 1. Data Import and Exploration\n",
    "# 2. Data Preprocessing\n",
    "# 3. Feature Engineering\n",
    "# 4. Model Training and Evaluation\n",
    "# 5. Prediction on Test Set\n",
    "# 6. Conclusions and Future Work\n",
    "\n",
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set up matplotlib for better visualizations\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "## 1. Data Import and Exploration\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('Project Data-20250507/train.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Training Data Shape:\", train_data.shape)\n",
    "train_data.head()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values in Training Data:\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "# Label distribution\n",
    "print(\"\\nLabel Distribution:\")\n",
    "label_counts = train_data['label'].value_counts().sort_index()\n",
    "print(label_counts)\n",
    "\n",
    "# Visualize label distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x='label', data=train_data, palette='viridis')\n",
    "plt.title('Label Distribution in Training Data')\n",
    "plt.xlabel('Sentiment Label (0: Bearish, 1: Bullish, 2: Neutral)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}', \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha = 'center', va = 'bottom',\n",
    "                xytext = (0, 5), textcoords = 'offset points')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text length analysis\n",
    "train_data['text_length'] = train_data['text'].apply(len)\n",
    "train_data['word_count'] = train_data['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Visualize text length distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(train_data['text_length'], kde=True, bins=50)\n",
    "plt.title('Distribution of Text Length (Characters)')\n",
    "plt.xlabel('Number of Characters')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(train_data['word_count'], kde=True, bins=50)\n",
    "plt.title('Distribution of Word Count')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize text length by sentiment\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.boxplot(x='label', y='word_count', data=train_data, palette='viridis')\n",
    "plt.title('Word Count by Sentiment Label')\n",
    "plt.xlabel('Sentiment Label (0: Bearish, 1: Bullish, 2: Neutral)')\n",
    "plt.ylabel('Word Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Function to extract most common words\n",
    "def get_most_common_words(text_series, n=20):\n",
    "    all_words = ' '.join(text_series).lower()\n",
    "    all_words = re.sub(r'[^\\w\\s]', '', all_words)\n",
    "    word_tokens = word_tokenize(all_words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in word_tokens if word not in stop_words and len(word) > 2]\n",
    "    return Counter(filtered_words).most_common(n)\n",
    "\n",
    "# Get most common words overall\n",
    "most_common_words = get_most_common_words(train_data['text'])\n",
    "words, counts = zip(*most_common_words)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(words), y=list(counts), palette='viridis')\n",
    "plt.title('Most Common Words in Tweets')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Word clouds for different sentiment labels\n",
    "def create_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                         max_words=100, contour_width=3, contour_color='steelblue').generate(text)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Word cloud for each sentiment category\n",
    "for label, sentiment in zip([0, 1, 2], ['Bearish', 'Bullish', 'Neutral']):\n",
    "    text = ' '.join(train_data[train_data['label'] == label]['text'])\n",
    "    create_wordcloud(text, f'Word Cloud for {sentiment} Tweets')\n",
    "\n",
    "## 2. Data Preprocessing\n",
    "\n",
    "# Define preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing URLs\n",
    "    3. Removing user mentions (@user)\n",
    "    4. Removing hashtags\n",
    "    5. Removing non-alphanumeric characters\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove hashtags (keeping the content without #)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters (keeping spaces)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, stop_words=None):\n",
    "    \"\"\"Remove stopwords from text\"\"\"\n",
    "    if stop_words is None:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"Apply stemming to text\"\"\"\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stemmed_text = [stemmer.stem(word) for word in word_tokens]\n",
    "    return ' '.join(stemmed_text)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Apply lemmatization to text\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "# Apply preprocessing to the training data\n",
    "train_data['cleaned_text'] = train_data['text'].apply(clean_text)\n",
    "train_data['text_no_stopwords'] = train_data['cleaned_text'].apply(remove_stopwords)\n",
    "train_data['stemmed_text'] = train_data['text_no_stopwords'].apply(stem_text)\n",
    "train_data['lemmatized_text'] = train_data['text_no_stopwords'].apply(lemmatize_text)\n",
    "\n",
    "# Display a random sample to check preprocessing effectiveness\n",
    "sample_indices = random.sample(range(len(train_data)), 3)\n",
    "for idx in sample_indices:\n",
    "    print(f\"Original: {train_data.loc[idx, 'text']}\")\n",
    "    print(f\"Cleaned: {train_data.loc[idx, 'cleaned_text']}\")\n",
    "    print(f\"No Stopwords: {train_data.loc[idx, 'text_no_stopwords']}\")\n",
    "    print(f\"Stemmed: {train_data.loc[idx, 'stemmed_text']}\")\n",
    "    print(f\"Lemmatized: {train_data.loc[idx, 'lemmatized_text']}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X = train_data['lemmatized_text']  # We'll use lemmatized text for now\n",
    "y = train_data['label']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Check the split\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Training label distribution: {y_train.value_counts().sort_index().tolist()}\")\n",
    "print(f\"Validation label distribution: {y_val.value_counts().sort_index().tolist()}\")\n",
    "\n",
    "## 3. Feature Engineering\n",
    "\n",
    "# 3.1 Bag of Words (BoW)\n",
    "bow_vectorizer = CountVectorizer(max_features=5000)\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_val_bow = bow_vectorizer.transform(X_val)\n",
    "\n",
    "print(f\"BoW vocabulary size: {len(bow_vectorizer.vocabulary_)}\")\n",
    "print(f\"BoW feature matrix shape: {X_train_bow.shape}\")\n",
    "\n",
    "# 3.2 TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "print(f\"TF-IDF vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"TF-IDF feature matrix shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# 3.3 Word2Vec\n",
    "# Train Word2Vec model on our corpus\n",
    "def get_tokenized_text(text_series):\n",
    "    return [word_tokenize(text.lower()) for text in text_series]\n",
    "\n",
    "tokenized_texts = get_tokenized_text(train_data['cleaned_text'])\n",
    "\n",
    "# Train the model\n",
    "w2v_model = Word2Vec(sentences=tokenized_texts, \n",
    "                    vector_size=100,\n",
    "                    window=5,\n",
    "                    min_count=2,\n",
    "                    workers=4,\n",
    "                    sg=1)  # sg=1 for skip-gram, sg=0 for CBOW\n",
    "\n",
    "# Function to create document embeddings from Word2Vec\n",
    "def get_document_vector(text, model, vector_size=100):\n",
    "    words = word_tokenize(text.lower())\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    # Average word vectors to get document vector\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Create document embeddings for train and validation sets\n",
    "X_train_w2v = np.array([get_document_vector(text, w2v_model) for text in X_train])\n",
    "X_val_w2v = np.array([get_document_vector(text, w2v_model) for text in X_val])\n",
    "\n",
    "print(f\"Word2Vec feature matrix shape: {X_train_w2v.shape}\")\n",
    "\n",
    "# 3.4 Pretrained GloVe embeddings\n",
    "# Load pretrained GloVe embeddings\n",
    "try:\n",
    "    glove_model = api.load('glove-twitter-25')  # 25-dimensional embeddings\n",
    "    \n",
    "    # Function to create document embeddings from GloVe\n",
    "    def get_glove_vector(text, model, vector_size=25):\n",
    "        words = word_tokenize(text.lower())\n",
    "        word_vectors = [model[word] for word in words if word in model]\n",
    "        \n",
    "        if len(word_vectors) == 0:\n",
    "            return np.zeros(vector_size)\n",
    "        \n",
    "        # Average word vectors to get document vector\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    \n",
    "    # Create document embeddings for train and validation sets\n",
    "    X_train_glove = np.array([get_glove_vector(text, glove_model) for text in X_train])\n",
    "    X_val_glove = np.array([get_glove_vector(text, glove_model) for text in X_val])\n",
    "    \n",
    "    print(f\"GloVe feature matrix shape: {X_train_glove.shape}\")\n",
    "except:\n",
    "    print(\"Could not load GloVe embeddings. Please ensure gensim is installed and internet is available.\")\n",
    "    X_train_glove, X_val_glove = None, None\n",
    "\n",
    "## 4. Model Training and Evaluation\n",
    "\n",
    "# 4.1 KNN with TF-IDF\n",
    "def train_and_evaluate_knn(X_train, X_val, y_train, y_val, feature_type):\n",
    "    # Train KNN model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = knn.predict(X_val)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"KNN with {feature_type} - Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "    print(f\"Weighted Precision: {precision_score(y_val, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"Weighted Recall: {recall_score(y_val, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"Weighted F1-score: {f1_score(y_val, y_pred, average='weighted'):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Bearish', 'Bullish', 'Neutral'],\n",
    "               yticklabels=['Bearish', 'Bullish', 'Neutral'])\n",
    "    plt.title(f'Confusion Matrix - KNN with {feature_type}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return knn\n",
    "\n",
    "# Train and evaluate KNN with TF-IDF features\n",
    "knn_tfidf = train_and_evaluate_knn(X_train_tfidf, X_val_tfidf, y_train, y_val, \"TF-IDF\")\n",
    "\n",
    "# Train and evaluate KNN with Word2Vec features\n",
    "knn_w2v = train_and_evaluate_knn(X_train_w2v, X_val_w2v, y_train, y_val, \"Word2Vec\")\n",
    "\n",
    "# 4.2 LSTM with Embeddings\n",
    "def train_and_evaluate_lstm(X_train, X_val, y_train, y_val):\n",
    "    # Tokenize text\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "    # Convert text to sequences\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "    \n",
    "    # Pad sequences\n",
    "    max_seq_length = 50\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_seq_length)\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=max_seq_length)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "    y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes=3)\n",
    "    \n",
    "    # Build LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=5000, output_dim=100, input_length=max_seq_length))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_pad, y_train_cat,\n",
    "        epochs=5,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val_pad, y_val_cat),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred_prob = model.predict(X_val_pad)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    print(\"LSTM Model - Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "    print(f\"Weighted Precision: {precision_score(y_val, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"Weighted Recall: {recall_score(y_val, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"Weighted F1-score: {f1_score(y_val, y_pred, average='weighted'):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Bearish', 'Bullish', 'Neutral'],\n",
    "               yticklabels=['Bearish', 'Bullish', 'Neutral'])\n",
    "    plt.title('Confusion Matrix - LSTM Model')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Train and evaluate LSTM model\n",
    "try:\n",
    "    lstm_model, tokenizer = train_and_evaluate_lstm(X_train, X_val, y_train, y_val)\n",
    "except Exception as e:\n",
    "    print(f\"Error training LSTM model: {e}\")\n",
    "    lstm_model, tokenizer = None, None\n",
    "\n",
    "# 4.3 Transformer-based approach (DistilBERT)\n",
    "def train_and_evaluate_transformer(X_train, X_val, y_train, y_val):\n",
    "    try:\n",
    "        # Initialize tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Tokenize data\n",
    "        def tokenize_data(texts, max_length=128):\n",
    "            return tokenizer(\n",
    "                texts.tolist(),\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "        \n",
    "        # Sample subset for demonstration (full training would require more resources)\n",
    "        sample_size = 500\n",
    "        indices = np.random.choice(len(X_train), size=sample_size, replace=False)\n",
    "        X_train_sample = X_train.iloc[indices]\n",
    "        y_train_sample = y_train.iloc[indices]\n",
    "        \n",
    "        val_indices = np.random.choice(len(X_val), size=100, replace=False)\n",
    "        X_val_sample = X_val.iloc[val_indices]\n",
    "        y_val_sample = y_val.iloc[val_indices]\n",
    "        \n",
    "        # Tokenize data\n",
    "        train_encodings = tokenize_data(X_train_sample)\n",
    "        val_encodings = tokenize_data(X_val_sample)\n",
    "        \n",
    "        # Convert labels to categorical\n",
    "        y_train_cat = tf.keras.utils.to_categorical(y_train_sample, num_classes=3)\n",
    "        y_val_cat = tf.keras.utils.to_categorical(y_val_sample, num_classes=3)\n",
    "        \n",
    "        # Create TensorFlow datasets\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(train_encodings),\n",
    "            y_train_cat\n",
    "        )).batch(16)\n",
    "        \n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(val_encodings),\n",
    "            y_val_cat\n",
    "        )).batch(16)\n",
    "        \n",
    "        # Load pretrained model\n",
    "        from transformers import TFAutoModelForSequenceClassification\n",
    "        \n",
    "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            \"distilbert-base-uncased\", \n",
    "            num_labels=3\n",
    "        )\n",
    "        \n",
    "        # Compile the model\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        metrics = [tf.keras.metrics.CategoricalAccuracy('accuracy')]\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=2  # More epochs would be better but this is for demonstration\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        val_logits = model.predict(dict(val_encodings)).logits\n",
    "        val_predictions = tf.argmax(val_logits, axis=1).numpy()\n",
    "        \n",
    "        # Evaluate\n",
    "        print(\"Transformer Model - Evaluation Metrics:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_val_sample, val_predictions):.4f}\")\n",
    "        print(f\"Weighted Precision: {precision_score(y_val_sample, val_predictions, average='weighted'):.4f}\")\n",
    "        print(f\"Weighted Recall: {recall_score(y_val_sample, val_predictions, average='weighted'):.4f}\")\n",
    "        print(f\"Weighted F1-score: {f1_score(y_val_sample, val_predictions, average='weighted'):.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_val_sample, val_predictions))\n",
    "        \n",
    "        return model, tokenizer\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error training transformer model: {e}\")\n",
    "        print(\"The transformer approach may require additional setup or GPU resources.\")\n",
    "        return None, None\n",
    "\n",
    "# Try to train transformer model (may not work in all environments)\n",
    "try:\n",
    "    transformer_model, transformer_tokenizer = train_and_evaluate_transformer(X_train, X_val, y_train, y_val)\n",
    "except Exception as e:\n",
    "    print(f\"Could not train transformer model: {e}\")\n",
    "    transformer_model, transformer_tokenizer = None, None\n",
    "\n",
    "## 5. Prediction on Test Set\n",
    "\n",
    "# Load test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "print(\"Test Data Shape:\", test_data.shape)\n",
    "test_data.head()\n",
    "\n",
    "# Preprocess test data with the same steps as training data\n",
    "test_data['cleaned_text'] = test_data['text'].apply(clean_text)\n",
    "test_data['text_no_stopwords'] = test_data['cleaned_text'].apply(remove_stopwords)\n",
    "test_data['stemmed_text'] = test_data['text_no_stopwords'].apply(stem_text)\n",
    "test_data['lemmatized_text'] = test_data['text_no_stopwords'].apply(lemmatize_text)\n",
    "\n",
    "# Choose the best performing model for final predictions\n",
    "# For this example, we'll use the KNN with TF-IDF features\n",
    "\n",
    "# Transform test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['lemmatized_text'])\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = knn_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'label': test_predictions\n",
    "})\n",
    "\n",
    "# Save predictions\n",
    "submission.to_csv('pred_XX.csv', index=False)\n",
    "print(\"Predictions saved to pred_XX.csv\")\n",
    "\n",
    "## 6. Conclusions and Future Work\n",
    "\n",
    "\"\"\"\n",
    "Model Comparison:\n",
    "- TF-IDF with KNN performed [describe performance]\n",
    "- Word2Vec with KNN performed [describe performance]\n",
    "- LSTM model performed [describe performance]\n",
    "- Transformer model performed [describe performance if available]\n",
    "\n",
    "Best Model: [Identify which model performed best]\n",
    "\n",
    "Future Improvements:\n",
    "1. Fine-tune hyperparameters using Grid Search or Random Search\n",
    "2. Experiment with different preprocessing techniques\n",
    "3. Try other transformer models like BERT or RoBERTa\n",
    "4. Implement ensemble methods combining multiple models\n",
    "5. Address class imbalance issues (if present)\n",
    "6. Augment training data\n",
    "7. Feature engineer additional features from the tweets (e.g., sentiment scores, entity recognition)\n",
    "8. Experiment with more advanced deep learning architectures\n",
    "\"\"\"\n",
    "\n",
    "# Display overview of experiments\n",
    "models = ['KNN (TF-IDF)', 'KNN (Word2Vec)', 'LSTM', 'Transformer']\n",
    "accuracies = [0.0, 0.0, 0.0, 0.0]  # Replace with actual values after running\n",
    "f1_scores = [0.0, 0.0, 0.0, 0.0]   # Replace with actual values after running\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, accuracies, width, label='Accuracy')\n",
    "plt.bar(x + width/2, f1_scores, width, label='F1-Score')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.0)\n",
    "\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i - width/2, v + 0.01, f'{v:.2f}', ha='center')\n",
    "\n",
    "for i, v in enumerate(f1_scores):\n",
    "    plt.text(i + width/2, v + 0.01, f'{v:.2f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca5933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
